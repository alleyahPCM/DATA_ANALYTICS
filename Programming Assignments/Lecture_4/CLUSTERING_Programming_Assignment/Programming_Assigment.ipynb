{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1181691e-d115-41a0-ae69-1b678d99a9b1",
   "metadata": {},
   "source": [
    "Alleyah Pauline C. Manalili"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b75fa-7a45-42ff-abe6-91ba6c3490ca",
   "metadata": {},
   "source": [
    "1. Read the Bernoulli Mixture Model Derivation.\n",
    "2. Read about Stochastic Expectation-Maximization (EM) Algorithm: https://www.sciencedirect.com/science/article/pii/S0167947320302504.\n",
    "3. From the given code, modify the EM algorithm to become a Stochastic EM Algorithm.\n",
    "4. Use the data from the paper: https://www.sciencedirect.com/science/article/abs/pii/S0031320322001753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df91c501-7b08-4070-84d9-541463ad4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "zoo = fetch_ucirepo(id=111)\n",
    "\n",
    "X = zoo.data.features\n",
    "y = zoo.data.targets \n",
    "zoo_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "\n",
    "zoo_df = zoo_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d9bb7c-39e7-45df-9baf-4b812d4fd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "\n",
    "class BernoulliMixturewSEM:\n",
    "    \n",
    "    def __init__(self, n_components, max_iter, batch_size=10, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            self.gamma = self.get_responsibilities(log_bernoullis)\n",
    "            self.remember_params()\n",
    "            self.get_Neff()\n",
    "            self.get_mu(self.x)\n",
    "            self.get_pi()\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def iterate_batches(self):\n",
    "        n_samples = len(self.x)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, n_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            yield self.x.iloc[batch_indices]\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "        self.old_mu = None\n",
    "        self.old_pi = None\n",
    "        self.old_gamma = None\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        epsilon = 1e-15\n",
    "        mu_place = np.clip(mu, epsilon, 1 - epsilon)\n",
    "        return np.tensordot(x, np.log(mu_place), (1,1))\n",
    "\n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self, batch):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, batch) / self.Neff[:, None]\n",
    "        \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84536992-31f4-43e9-bc60-c53b7c93c58f",
   "metadata": {},
   "source": [
    "5. Perform categorical clustering using the Bernoulli Mixture Model with Stochastic EM Algorithm.\n",
    "6. Compare its performance with K-Modes Algorithm using Folkes-Mallows Index, Adjusted Rand Index, and Normalized Mutual Information Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289d7fbb-fb3d-4b8a-9dd9-d6319e5d8f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>FMI</th>\n",
       "      <th>ARI</th>\n",
       "      <th>NMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zoo_df</td>\n",
       "      <td>BernoulliMixturewSEM</td>\n",
       "      <td>0.674122</td>\n",
       "      <td>0.447982</td>\n",
       "      <td>0.579111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zoo_df</td>\n",
       "      <td>KModes</td>\n",
       "      <td>0.674122</td>\n",
       "      <td>0.447982</td>\n",
       "      <td>0.579111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset             Algorithm       FMI       ARI       NMI\n",
       "0  zoo_df  BernoulliMixturewSEM  0.674122  0.447982  0.579111\n",
       "1  zoo_df                KModes  0.674122  0.447982  0.579111"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "def encode_categorical(df):\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = df.copy()\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            encoded_df[column] = encoder.fit_transform(df[column])\n",
    "    return encoded_df\n",
    "\n",
    "encoded_datasets = {}\n",
    "df = zoo_df.copy()\n",
    "encoded_datasets['zoo_df'] = encode_categorical(df)\n",
    "\n",
    "def evaluate_clustering(dataset_name, algorithm, **kwargs):\n",
    "    dataset = encoded_datasets[dataset_name]\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    true_labels = dataset.iloc[:, -1]\n",
    "    \n",
    "    if algorithm == 'BernoulliMixturewSEM':\n",
    "        model = BernoulliMixturewSEM(**kwargs)\n",
    "        model.fit(X)\n",
    "        labels = model.predict(X)\n",
    "    elif algorithm == 'KModes':\n",
    "        km = KModes(**kwargs)\n",
    "        labels = km.fit_predict(X)\n",
    "    else:\n",
    "        raise ValueError(\"Algorithm not supported.\")\n",
    "    \n",
    "    fmi, ari, nmi = None, None, None\n",
    "    \n",
    "    if true_labels is not None:\n",
    "        fmi = fowlkes_mallows_score(true_labels, labels)\n",
    "        ari = adjusted_rand_score(true_labels, labels)\n",
    "        nmi = normalized_mutual_info_score(true_labels, labels)\n",
    "    \n",
    "    return fmi, ari, nmi\n",
    "\n",
    "results = {}\n",
    "algorithms = ['BernoulliMixturewSEM', 'KModes']\n",
    "dataset_name = 'zoo_df' \n",
    "results[dataset_name] = {}\n",
    "for algorithm in algorithms:\n",
    "    if algorithm == 'BernoulliMixturewSEM':\n",
    "        kwargs = {'n_components': 2, 'max_iter': 100}\n",
    "    elif algorithm == 'KModes':\n",
    "        kwargs = {'n_clusters': 2, 'max_iter': 100}\n",
    "    fmi, ari, nmi = evaluate_clustering(dataset_name, algorithm, **kwargs)\n",
    "    results[dataset_name][algorithm] = {'FMI': fmi, 'ARI': ari, 'NMI': nmi}\n",
    "\n",
    "data = []\n",
    "\n",
    "for dataset_name, algorithms in results.items():\n",
    "    if dataset_name == 'zoo_df':\n",
    "        for algorithm, metrics in algorithms.items():\n",
    "            data.append([dataset_name, algorithm, metrics['FMI'], metrics['ARI'], metrics['NMI']])\n",
    "\n",
    "results_df = pd.DataFrame(data, columns=['Dataset', 'Algorithm', 'FMI', 'ARI', 'NMI'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a63584-ea38-4b72-9e3b-045ba01661b8",
   "metadata": {},
   "source": [
    "7. Compare and contrast the performances, and explain what is happening (i.e. why is FMI always higher than ARI and NMI? Why is ARI and NMI low compared to FMI? etc.)\n",
    "8. Write the report in Latex, push to your github with the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696da4e6-2b20-48c9-9230-a8f0c5264b25",
   "metadata": {},
   "source": [
    "Based on the output, the Folkes-Mallows Index (FMI) with 0.674122 is higher than Adjusted Rand Index (ARI) with 0.447982 and Normalized Mutual Information Score (NMI) with 0.579111. FMI is higher because it focuses on cluster similarity, while ARI and NMI take into account both similarity and dissimilarity. FMI may also yield a higher performance despite noise or intra-cluster variations in comparison to ARI and NMI. Moreover, the algorithms implemented, Bernoulli Mixture with Stochastic EM and KModes, are more compatible with FMI due to properties and assumptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
